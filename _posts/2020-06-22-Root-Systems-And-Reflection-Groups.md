---
title: 'Root Systems and Reflection Groups'
date: 2020-06-22
permalink: /posts/RootSystemsAndReflectionGroups/
tags:
  - Lie Algebras
  - Group Theory
---

I first came across the notion of root systems from [Humphreys](https://books.google.com/books/about/Introduction_to_Lie_Algebras_and_Represe.html?id=gCUlAQAAIAAJ). Leading up to that section, Humphreys presents the root space decomposition of a finite-dimensional semisimple Lie algebra, and at the end he notes some interesting properties that these roots have. These proeprties seem rather forced observatio a priori, but upon learning about the general theory of root systems, it becomes rather amazing that root systems do indeed arise in the theory of semisimple Lie algebras.

This connection between something as algebraic like Lie algebras with something as geometric and, to an extent, combinatorial like root systems has deep consequences. Namely, from classifying (reduced crystallographic) root systems, one can classify all possible simple Lie algebras; see [here](https://almosttrivial.github.io/posts/SimpleClassification/) for details.

However, root systems arise beyond just this classical study, appearing in more general constructions like affinization, superification, and quantization of Lie algebras. They also have deep connections to a family of groups known as Coxeter groups, and this is the perspective I plan on expanding upon here and in future blog posts. In particular, I will predominantly be following another book of [Humphreys](https://books.google.com/books/about/Reflection_Groups_and_Coxeter_Groups.html?id=ODfjmOeNLMUC).

The theory of root systems takes place in a real Euclidean space \\(V\\) equipped with a positive-definite symmetric bilinear form \\((\lambda,\mu)\\). In such a space, one can associate to any vector \\(\alpha\in V\\) the linear operator of **reflection** along \\(\alpha\\) i.e. the linear map

\begin{equation\*}
   s\_{\alpha}(\lambda) := \lambda - \frac{2(\lambda,\alpha)}{(\alpha,\alpha)}\alpha
\end{equation\*}

Note that this operator fixes pointwise every vector orthogonal to \\(\alpha\\) and send \\(\alpha\mapsto-\alpha\\). Moreover, this map is an orthogonal involution i.e.

\begin{equation\*}
   (s\_{\alpha}(\lambda),s\_{\alpha}(\mu)) = (\lambda,\mu),\quad\quad s\_{\alpha}^{2} = \text{id}
\end{equation\*}

We call a finite group generated by such simple reflections a **finite refletion group**. The reader is likely aware of at least the following family of finite reflection groups: the symmetric group on \\(n\\) letters! Indeed, there is a subgroup of orthogonal transformations on \\(\mathbb{R}^{n}\\) isomorphic to \\(S\_{n}\\). Namely, map each element of \\(S\_{n}\\) to the corresponding permutation on the coordinates; this clearly has an image in \\(O(V)\\) and is one-to-one. But the transposition \\((i,i +1)\\) generates \\(S\_{n}\\), hence their image in \\(O(V)\\) will also generate the image. If we denote a basis for \\(\mathbb{R}^{n}\\) by \\(\varepsilon\_{1},\dots,\varepsilon\_{n}\\), then the orthogonal transformation corresponding to \\((i,i + 1)\\) sends \\(\varepsilon\_{i} - \varepsilon\_{i+1}\\) to its negative, and fixes pointwise the corresponding hyperplane consisting of all vectors whose \\(i\\)-th and \\((i + 1)\\)-th coordinates are equal.

Now, the astute reader might be wondering what groups have to do with something I said would be more geometric and combinatorial. To this end, denote a finite reflection group by \\(W\\). Then recall that each simple reflection \\(s\_{\alpha}\\) fixes a hyperplane and reflects the line spanned by \(\alpha\\) orthogonal to it. Thus, each simple reflection determines such a line, and the marvelous thing is that the entire group \\(W\\) permutes these lines! That is

**Proposition** If \\(\phi\in O(V)\\) and \\(\alpha\in V\backslash\\{0\\}\\), then \\(\phi\circ s\_{\alpha}\circ\phi^{-1} = s\_{\phi(\alpha)}\\). In particular, if one takes \\(\phi \in W\\), then as a corollary \\(s\_{\phi(\alpha)}\in W\\) whenever \\(s\_{\alpha}\in W\\).

**Proof** In order to show \\(\phi\circ s\_{\alpha}\circ\phi^{-1} = s\_{\phi(\alpha)}\\) we must show that both sides map the same vector to the same image. To this end, consider \\(x\in H\_{\phi(\alpha)}\\), the hyperplane orthgonal to \\(\phi(\alpha)\\). For such an \\(x\\) we have \\(0 = (x,\phi(\alpha)) = (\phi^{-1}(x),\alpha)\\), where the second equality comes from the fact that \\(\phi\\) and hence \\(\phi^{-1}\\) are orthogonal. Thus

\begin{equation\*}
   \phi(s\_{\alpha}(\phi^{-1}(x))) = \phi(\phi^{-1}(x)) = x = s\_{\phi(\alpha)}(x)
\end{equation\*}

Lastly, for \\(\phi(\alpha)\\) one also has

\begin{equation\*}
   \phi(s\_{\alpha}(\phi^{-1}(\phi(\alpha)))) = \phi(s\_{\alpha}(\alpha)) = -\phi(\alpha) = s\_{\phi(\alpha)}(\phi(\alpha))
\end{equation\*}
from which the result follows. \\(\Box\\)

While finite reflection groups permute the uniquely determined lines, they do not necessarily determine any particular vectors along it. But if we select a pair of vectors along this line of the same length, one in each *direction along the line*, then this collection of vectors will be stable under the action of \\(W\\). We have thus reduced our considerations from a whole line, to simply a pair of equal length vectors of opposite orientation for each line. This is the geometric object we axiomatize and call a root system!

Let \\(\Phi\\) be a finite set of nonzero vectors in \\(V\\) such that

1. \\(\Phi\cap\mathbb{R}\alpha = \\{\alpha,-\alpha\\}\\) for all \\(\alpha\in\Phi\\).
2. \\(s\_{\alpha}(\Phi) = \Phi\\) for all \\(\alpha\in\Phi\\).

Then define \\(W\\) to be the group generated by all such reflections \\(s\_{\alpha}\\). We call \\(\Phi\\) a **root system** with associated reflection group \\(W\\). The elements of \\(\Phi\\) are called **roots**. The astute reader might wonder how uniquely \\(W\\) and \\(\Phi\\) determine one another. In fact, not that much! That is, every finite reflection group leads to a root system, but there may be several which generate the same group. Moreover, the group \\(W\\) coming from a root system is always finite because it can be embedded within the symmetric group on \\(\Phi\\), which is finite since \\(\Phi\\) is assumed to be finite.

To those familiar with root systems from the context of Lie algebra theory, there two additional axioms taken as part of the definition. The first concerns whether the roots span the ambient space and the second pertains to certain inner products of roots being integral. It turns out that several of the results for root systems, say as presented in [Humphreys](https://books.google.com/books/about/Introduction_to_Lie_Algebras_and_Represe.html?id=gCUlAQAAIAAJ), follow without needing these two additional criteria. As we will see later (perhaps in another blog post), root systems arising in Lie theory are instances of **crystallographic root systems**.

I will likely have future blog posts detailing more about this theory, but risking that this blog post might get a little long, I will discuss one important construction that provides insight in to both a root system and its reflection group: simple systems.

In particular, note that the cardinality of a root system might be rather large compared to the dimension of the ambient vector space in which it lives. This suggests looking for some subset of linearly independent roots from which the whole root system can be obtained from. However, one actually can ask for something a bit more special than this sort of *basis*. Instead, one would like such a collection which then further allows one to partition the root system into *positive* roots and *negative roots*.

First, one calls a subset \\(\Pi\\) of \\(\Phi\\) a **positive system** if it consists of all the roots which are *positive* relative to some *total ordering of \\(V\\)*. Because every root appears with its negative, a total ordering on \\(V\\) induces a partition of \\(\Phi\\) into the union of \\(\Pi\\) and \\(-\Pi\\), the latter being called a **negative system**. 

Call a subset \\(\Delta\\) of \\(\Phi\\) a **simple system** if \\(\Delta\\) is a basis for the \\(\mathbb{R}\\)-span of \\(\Phi\\) such that every root is a linear combination of \\(\Delta\\) with coefficients *all of the same sign*. The elements of \\(\Delta\\) are called **simple roots**. While \\(\Delta\\) being a basis for \\(\text{Span}\_{\mathbb{R}}\Phi\\) is already a bit of a tall order, seeing as how \\(\Phi\\) might not even necessarily span \\(V\\), the latter requirement of same sign coefficients is even more demanding. The surprising thing though is that simple systems actually exist!

**Theorem**<ol type="a">
  <li>If \\(\Delta\\) is a simple system in \\(\Phi\\), then there is a unique positive system containing \\(\Delta\\).</li>
  <li>Every positive system \\(\Pi\\) in \\(\Phi\\) contains a unique simple system.</li>
</ol>

**Proof** 
<ol type = "a">
  
  <li>  </li>
  
</ol>
